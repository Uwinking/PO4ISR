
提示词 + RAG + 自反思迭代的整体流程

#### 1. 离线准备

1. **记忆文本拼装**：把 2 566 名历史用户的「长期‑短期行为」各写成一行：
   *Long‑term history: …  Short‑term history: …*
2. **向量化**：用大模型的文本嵌入接口，得到 3072 维向量 → 记忆向量库。
3. **持久化**：保存

   * 向量 (n = 2566, d = 3072)
   * 对应原始提示词 JSON（后续检索到坐标即可取出文本）。

---
#### 2. RAG 注入：为错误样本找“相似用户”

1. 把该样本自身的长/短期历史拼成一句并向量化。
2. 在记忆库做余弦检索：

   * 去掉相似度最高的一条（避免是自己或极端异常）。
   * 取次高的两条 → 得到 **user 1 / user 2** 的行为提示词。
3. 把这两段文本插入专用模板，拼到主提示词前 → 形成 **RAG‑增强 Prompt**。

---
#### 3. 第一次排名（RAG‑增强 Prompt）

* 在训练集上直接用 **初始提示词** 让模型对 20 候选进行排名。
* 若目标商品排名 ≤ 10 ⇒ 视为正确，直接让当前提示词多生成四个效果一样的提示词集，直接用训练数据集对这五个提示词进行评分；否则记为 **“首轮错误”** 样本。

---

#### 4. 第二次排名（含相似用户 Prompt）

* 对首轮错误样本分析喜欢和不喜欢目标商品的原因和为什么排名错误的原因，再排一次。
* 若目标仍排名 > 10 → 进入 **errors\_list**（说明二次仍失败）。
* 同时从第一次回答中解析 `<INTENT>…</INTENT>` 作为 **preference** 记录。（这个是用户偏好，告诉当前用户的偏好，同时分析喜欢和不喜欢目标商品的原因）

---
（由于第二次仍然错误的样本个数很少，我把第一次和第二次都错误的样本都加入到了errorlist，虽然样本数会有重复，但是是否可以说多关注第二次仍然错误的样本 变向增加第二次错误样本的”权重“）
#### 5. 自反思生成改进提示词（Expand 阶段）

* 针对 `errors_list` 中的每个样本：

  * 已知每个错误样本当前的用户偏好，分析错误原因
  * 然后用这些原因生成改进提示词及其同义变体 → 得到一批候选提示词。

---

#### 6. UCB‑Bandit 评估 & Beam‑Search（Select 阶段）

1. 对所有候选提示词用小批训练样本估算 HR/NDCG 奖励（UCB 方式）。
2. 取 **得分最高的前 5 条** 更新为新的 `beam_candidate`。
3. 重复“生成‑选择”若干深度（`search_depth`）实现 Beam‑Search。

---

#### 7. 验证集选最优 Prompt

* 将最后一层 `beam_candidate` 的 5 个提示词放在 **验证集** 上完整评估。
* 选择分数最高者作为 **new\_prompt**（即最终迭代得到的最优提示词）。

---

##### 流程摘要

> 原始 Prompt → 首轮排名，同时注入相似用户(RAG)→ 取错例  → 自反思分析喜欢和不喜欢目标商品的原因，二次排名 → 仍错例 →
> 自反思生成新 Prompt集 → UCB/Beam 搜索 → 验证集挑优 → 得到最终最优 Prompt
